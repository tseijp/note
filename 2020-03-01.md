---
tags: [Python, PyTorch, GAN]
title: few-shot-vid2vid
description: few-shot-vid2vid
authors: tsei
image: https://github.com/tseijp.png
hide_table_of_contents: false
---

# few-shot-vid2vid
自分用のメモです。英弱なので、ざっくり読んで、
重要そうな部分をひたすらgoogle先生に聞いて読んでます。
GAN全く詳しくないので間違ってる箇所多いと思います。

__REF__
* [arXiv](https://arxiv.org/abs/1910.12713)
* [youtube](https://youtu.be/8AZBuyEuDqc)
* [github](https://github.com/NVlabs/few-shot-vid2vid)

# Introdunction
1. vid2vid: 人間のポーズやマスクのセマンティックを入力ビデオとし、フォトリアルなビデオに変換するタスク。大きな制限。
    1.  生成したい人やシーンの多数の画像がトレーニングに必要
    1. トレーニングした人の動画のみ合成できます。
1.  `Typically, to obtain such a model, one begins with collecting a training dataset for the target task.` !?!?
1. 新たな入力セマンティックビデオへの一般化は不十分!?
    1. データセットに含まれていない被験者の動画を生成するなど目に見えないドメインに一般化できるモデルを目指すべき
    1. vid2vidモデルが少数の画像だけで一般化できない場合、多くの画像を収集する必要
1. attention(注目の？)メカニズムを使った新しいNetwork weight生成モジュールを介して、一般化を実現
1. vid2vidがtrainデータと同じビデオのみ合成できるのに対し、このモデルでは,ビデオ合成メカニズムを動的に構成
1. サンプル画像を使用してネットワークの重みを生成するモジュールをトレーニング
1. 生成モジュールの学習を促進するため,the learning objective function(学習目的関数？)を慎重に設計


## Related work
### GAN
1. GANs [[13][13]]: few-shot vid2vid modelの基盤。ノイズ分布からサンプルを変換して出力
1. conditional GAN framework[[13][13], [42][42], [32][32], [14][14], [25][25]]:
   入力データに基づいて出力を生成することで、出力をより柔軟に制御。入力データ形式はさまざま..。
    1. images [[22][22], [68][68], [30][30], [41][41]]
    1. categorical labels [39, 35, 65, 4]
    1. textual descriptions [43, 66, 62]
    1. videos [7, 12, 57, 67]: 今回使用したがこれは入力動画が一つ。

[13]: https://arxiv.org/abs/1612.05424
[14]: https://arxiv.org/abs/1704.00028
[22]: https://arxiv.org/abs/1611.07004
[25]: https://arxiv.org/abs/1812.04948
[30]: https://arxiv.org/abs/1703.00848
[32]: https://arxiv.org/abs/1606.07536
[41]: https://arxiv.org/abs/1903.07291
[42]: https://arxiv.org/abs/1511.06434
[68]: https://arxiv.org/abs/1703.10593

### Video generative models
1. unconditional video synthesis models [54, 45, 51]: ランダムノイズから動画に変換
2. future video prediction models [48, 24, 11, 34, 33, 63, 55, 56, 10, 53, 29, 27, 18, 28, 16, 40]:
   未来のビデオフレームを観察されたものに基づいて生成
3. vid2vid models [57, 7, 12, 67]: semantic入力動画からフォトリアルな動画に変換。
   見えないドメイン（学習していないモデル）のビデオを合成できることが新規性.


## Adaptive networks
重みの一部が入力データに基づいて動的に計算されるnetworks.
通常のネットワークとは異なるinductive bias(誘導バイアス?)があります。

  1. sequence modeling [15],
  1. image filtering [23, 59, 49]
  1. frame interpolation [38, 37]
  1. neural architecture search [64]:aply

### Human pose transfer
1. 異なるポーズの人間の画像を利用することにより、見えないポーズの人間を合成
1. Human pose transferの方法は人体の事前優先順位を主に利用します。
  1. 身体部分モデリング[1]
  1. 人間の表面ベースの座標マッピング[36]
1. 今回は入力semanticビデオのみ使用し、特定の人体優先順位は使用しません。
   ストリートシーンビデオ合成などの他のvid2vidタスクに同じモデルを直接使用できます。。。！？！？え
1. (既存の人間のポーズ転送方法は主に静止画像合成用に設計されており、
    問題の時間的側面を考慮していません。より時間的に一貫した結果をレンダリングする)

vid2vid model can convert a sequence of input semantic images
$s^T_1 : s_1,s_2,...s_T$ to a sequence of output images $x^T_1=x_1,x_2,...x_T$
sequential generative model given by

$$
x_t = F(x^{t-1}_{t-r}, s^t_{t-r}) = (1-m_t) \kentengCircle w_{t-1}(x_{t-1})+m_t \kentengCircle h_t
$$

- $m_t = M_{heta_M}(x^{t-1}_{t-r},s^t_{t-r})$ is  a soft occlution map
- $w_{t-1} = W_heta_W(x^{t-1}_{t-r},s^t_{t-r})$ is the optical flow warped version of the last generated images
- $h_t=H_heta_H(x^{t-1}_{t-r},s^t_{t-r})$ is the synthesized intermediate image



# few shot vid2vid synthesis
fewshot vid2vid model convert novel input semantic videos,
K-shot example image and semantic image ${e_1,e_2,...e_K},{S_{e_1},S_{e_2},...S_{e_K}}$ .

$$
x_t = F(x^{t-1}_{t-au}, s^t_{t-au},{e_1,e_2,...e_K}, {S_{e_1}, S_{e_2},...S_{e_K})
$$

we propose a network weight $heta$ generation module $E$ for the image synthesis network $H$.

$$
heta_H = E(x_{t-1}^{t-au}, s^t_{t-au},{e_1,e_2,...e_K},{s_{e_1},s_{e_2}})
$$

few-shot vid2vid framework based on Wang et al. [57],], which is the state-of-the-art. for the vid2vid task .we adopt the SPADE generator [41]
